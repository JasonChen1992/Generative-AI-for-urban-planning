Generative AI for urban planning: synthesizing street-view imagery with diffusion models

Street-view imagery has become an essential data source widely used in urban studies on visual perception, environmental exposure, and social behavior. Prior research has used such imagery to assess greenery, walkability, safety, and overall streetscape quality. Yet these efforts primarily focus on analyzing existing environments, with limited tools available to imagine or visualize alternative urban scenarios. Recent advances in generative artificial intelligence (GenAI), particularly diffusion models, now make it possible to synthesize realistic urban imagery. However, their integration into urban planning remains limited, particularly in generating street-level imagery that aligns with specific design intentions and constraints. To address this gap, this study introduces diffusion models into the urban planning context by integrating text prompts and spatial constraints into a ControlNet-based architecture, enhanced with semantic segmentation and object detection for fine-grained, multi-element control in generating street-view imagery. Experiments using street-view imagery from two contrasting urban contexts, Chicago and Orlando, demonstrate that the generated images respond predictably to prompt modifications while preserving spatial realism. The model supports both single-element and multi-element adjustments and reveals interpretable trade-offs between visual components. These results showcase the potential of controllable generative models to assist planners and stakeholders in visualizing alternative streetscape scenarios in a flexible and intuitive way.

This repository is forked from the original [ControlNet](https://github.com/lllyasviel/ControlNet)
